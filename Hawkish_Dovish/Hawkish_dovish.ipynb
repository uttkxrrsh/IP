{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import regex as re\n",
    "from nltk import word_tokenize \n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from dictionary import *\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# filename is test.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uttkarsh/IP\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/uttkarsh/IP/')\n",
    "print(os.getcwd())\n",
    "def get_text(file):\n",
    "    text = ''\n",
    "    with open(file, 'rb') as pdfFileObj:\n",
    "        pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "        num_pages = len(pdfReader.pages)\n",
    "        for i in range(num_pages):\n",
    "            pageObj = pdfReader.pages[i]\n",
    "            text += pageObj.extract_text()\n",
    "        text = text.replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "def get_string_from_list(list):\n",
    "    string = ''\n",
    "    for i in range(len(list)):\n",
    "        string += list[i]\n",
    "        if i != len(list)-1:\n",
    "            string += ' '\n",
    "    return string\n",
    "\n",
    "def filter(text):\n",
    "    token = word_tokenize(text)\n",
    "    token = [word.lower() for word in token]\n",
    "    token = [word for word in token if word.isalpha()]\n",
    "    token = [word for word in token if (word == 'down' or not word in stop_words )]\n",
    "    # remoev single character\n",
    "    token = [word for word in token if len(word) > 1]\n",
    "    # remove th\n",
    "    token = [word for word in token if word != 'th']\n",
    "    for i in range(len(token)):\n",
    "        if token[i] == 'per' and token[i+1] == 'cent':\n",
    "            token[i] = 'percent'\n",
    "            token[i+1] = ''\n",
    "    token = [word for word in token if word != '']\n",
    "    return token\n",
    "\n",
    "def get_bigram(token):\n",
    "    token = [stemmer.stem(word) for word in token]\n",
    "    bigram = ngrams(token, 2)\n",
    "    return list(bigram)\n",
    "\n",
    "def get_trigram(token):\n",
    "    token = [stemmer.stem(word) for word in token]\n",
    "    trigram = ngrams(token, 3)\n",
    "    return list(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchAndCount_single(bigram_list, term, hdlist):\n",
    "    term = stemmer.stem(term)\n",
    "    count = 0\n",
    "    for i in range(len(bigram_list)):\n",
    "        if term in bigram_list[i]:\n",
    "            for j in range(len(hdlist)):\n",
    "                if i-7 < 0:\n",
    "                    for k in range(i+7):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "                elif i+7 > len(bigram_list):\n",
    "                    for k in range(i, len(bigram_list)):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "                else:\n",
    "                    for k in range(i-7, i+7):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def searchAndCount_double(bigram_list, term1, term2, hdlist):\n",
    "    term1 = stemmer.stem(term1)\n",
    "    term2 = stemmer.stem(term2)\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(bigram_list)):\n",
    "        if term1 in bigram_list[i] and term2 in bigram_list[i]:\n",
    "            for j in range(len(hdlist)):\n",
    "                if i-7 < 0:\n",
    "                    for k in range(i+7):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "                elif i+7 > len(bigram_list):\n",
    "                    for k in range(i, len(bigram_list)):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "                else:\n",
    "                    for k in range(i-7, i+7):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "    return count\n",
    "\n",
    "def searchAndCount_triple(trigram_list, term1, term2, term3, hdlist):\n",
    "    term1 = stemmer.stem(term1)\n",
    "    term2 = stemmer.stem(term2)\n",
    "    term3 = stemmer.stem(term3)\n",
    "    count = 0\n",
    "    for i in range(len(trigram_list)):\n",
    "        if term1 in trigram_list[i] and term2 in trigram_list[i] and term3 in trigram_list[i]:\n",
    "            for j in range(len(hdlist)):\n",
    "                if i-7 < 0:\n",
    "                    for k in range(i+7):\n",
    "                        if hdlist[j] in trigram_list[k]:\n",
    "                            count += 1\n",
    "                elif i+7 > len(trigram_list):\n",
    "                    for k in range(i, len(trigram_list)):\n",
    "                        if hdlist[j] in trigram_list[k]:\n",
    "                            count += 1\n",
    "                else:\n",
    "                    for k in range(i-7, i+7):\n",
    "                        if hdlist[j] in trigram_list[k]:\n",
    "                            count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is  a file named 'Governer Speeches' which contain all the folders with speeches in them. I want to open the files on by one and run the code on them.\n",
    "# after running the code I want to make dictionary of the net hawkishness of each speech and the date of the speech. and save it as a json file with the same name as the file name i run the code\n",
    "# go to the folder 'Governer Speeches' and for all the subfolders in it open eaach pdf file in it and run the code\n",
    "\n",
    "# opening each folder\n",
    "os.chdir('C:/Users/karti/OneDrive/Desktop/IP/IP')\n",
    "\n",
    "def get_folder_names():\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    folder_names = glob.glob(cwd + '/Governor Speeches/*')\n",
    "    return folder_names\n",
    "\n",
    "# opening each file in each folder\n",
    "def make_wordcloud_image():\n",
    "    folder_names = get_folder_names()\n",
    "    print(folder_names)\n",
    "    # show this as tqdm\n",
    "    for i in range(len(folder_names)):\n",
    "        print(folder_names[i])\n",
    "        os.chdir(folder_names[i])\n",
    "        file_names = glob.glob('*.pdf')\n",
    "        for file_name in file_names:\n",
    "            print(file_name)\n",
    "            text = get_text(file_name)\n",
    "            file_name = file_name[:-4] # removing .pdf from the file name\n",
    "            if os.path.exists(file_name + '_trigram' + '.json'):\n",
    "                os.remove(file_name + '_trigram' + '.json')\n",
    "            if os.path.exists(file_name + '_bigram' + '.json'):\n",
    "                os.remove(file_name + '_bigram' + '.json')\n",
    "            token = filter(text)\n",
    "            N = len(token)\n",
    "            string_list = get_string_from_list(token)\n",
    "            bigram_list = get_bigram(token)\n",
    "            trigram_list = get_trigram(token)\n",
    "            wordcloud = WordCloud(width=1600, height=800, max_font_size=200).generate(string_list)\n",
    "            wordcloud.to_file(file_name + '.png')\n",
    "\n",
    "            cpi_hawkish = searchAndCount_single(bigram_list, 'cpi', ConsumerPricesInflation_Hawkish) + searchAndCount_double(bigram_list, 'cpi', 'inflation', ConsumerPricesInflation_Hawkish) + searchAndCount_triple(trigram_list, 'consumer', 'prices', 'inflation', ConsumerPricesInflation_Hawkish) + searchAndCount_single(bigram_list, 'inflation', ConsumerPricesInflation_Hawkish)\n",
    "            cpi_dovish = searchAndCount_single(bigram_list, 'cpi', ConsumerPricesInflation_Dovish) + searchAndCount_double(bigram_list, 'cpi', 'inflation', ConsumerPricesInflation_Dovish) + searchAndCount_triple(trigram_list, 'consumer', 'prices', 'inflation', ConsumerPricesInflation_Dovish) + searchAndCount_single(bigram_list, 'inflation', ConsumerPricesInflation_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            coreinflation_hawkish = searchAndCount_double(bigram_list, 'core', 'inflation', ConsumerPricesInflation_Hawkish)\n",
    "            coreinflation_dovish = searchAndCount_double(bigram_list, 'core', 'inflation', ConsumerPricesInflation_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            ip_inflation_hawkish = searchAndCount_single(bigram_list, 'inflation', InflationPressure_Hawkish)\n",
    "            ip_inflation_dovish = searchAndCount_single(bigram_list, 'inflation', InflationPressure_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            cs_hawkish = searchAndCount_double(bigram_list, 'consumer', 'spending', ConsumerSpending_Hawkish)\n",
    "            cs_dovish = searchAndCount_double(bigram_list, 'consumer', 'spending', ConsumerSpending_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            e_hawkish = searchAndCount_single(bigram_list, 'economic', EconomicActivity_Hawkish)\n",
    "            e_dovish = searchAndCount_single(bigram_list, 'economic', EconomicActivity_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            ea_hawkish = searchAndCount_double(bigram_list, 'economic', 'activity', EconomicActivity_Hawkish)\n",
    "            ea_dovish = searchAndCount_double(bigram_list, 'economic', 'activity', EconomicActivity_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            ru_hawkish = searchAndCount_double(bigram_list, 'resource', 'utilisation', ResourceUtilization_Hawkish) + searchAndCount_double(bigram_list, 'resource', 'utilization', ResourceUtilization_Hawkish)\n",
    "            ru_dovish = searchAndCount_double(bigram_list, 'resource', 'utilisation', ResourceUtilization_Dovish) + searchAndCount_double(bigram_list, 'resource', 'utilization', ResourceUtilization_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            em_hawkish = searchAndCount_single(bigram_list, 'employment', Employment_Hawkish)\n",
    "            em_dovish = searchAndCount_single(bigram_list, 'employment', Employment_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            lm_hawkish = searchAndCount_single(bigram_list, 'labour', LaborMarket_Hawkish) + searchAndCount_double(bigram_list, 'labour', 'market', LaborMarket_Hawkish)\n",
    "            lm_dovish = searchAndCount_single(bigram_list, 'labour', LaborMarket_Dovish) + searchAndCount_double(bigram_list, 'labour', 'market', LaborMarket_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            uem_hawkish = searchAndCount_single(bigram_list, 'unemployment', Unemployment_Hawkish)\n",
    "            uem_dovish = searchAndCount_single(bigram_list, 'unemployment', Unemployment_Dovish)\n",
    "\n",
    "            # make a dictionary of all above values and dump it in a json file\n",
    "            dict = {'cpi_hawkish': cpi_hawkish, 'cpi_dovish': cpi_dovish, 'coreinflation_hawkish': coreinflation_hawkish, 'coreinflation_dovish': coreinflation_dovish, 'ip_inflation_hawkish': ip_inflation_hawkish, 'ip_inflation_dovish': ip_inflation_dovish, 'cs_hawkish': cs_hawkish, 'cs_dovish': cs_dovish, 'e_hawkish': e_hawkish, 'e_dovish': e_dovish, 'ea_hawkish': ea_hawkish, 'ea_dovish': ea_dovish, 'ru_hawkish': ru_hawkish, 'ru_dovish': ru_dovish, 'em_hawkish': em_hawkish, 'em_dovish': em_dovish, 'lm_hawkish': lm_hawkish, 'lm_dovish': lm_dovish, 'uem_hawkish': uem_hawkish, 'uem_dovish': uem_dovish}\n",
    "            total_hawk = cpi_hawkish + coreinflation_hawkish + ip_inflation_hawkish + cs_hawkish + e_hawkish + ea_hawkish + ru_hawkish + em_hawkish + lm_hawkish + uem_hawkish\n",
    "            total_dovish = cpi_dovish + coreinflation_dovish + ip_inflation_dovish + cs_dovish + e_dovish + ea_dovish + ru_dovish + em_dovish + lm_dovish + uem_dovish\n",
    "            if(total_hawk + total_dovish == 0):\n",
    "                if(os.path.exists(file_name + '.pdf')):\n",
    "                    os.remove(file_name + '.pdf')\n",
    "                if(os.path.exists(file_name + '.png')):\n",
    "                    os.remove(file_name + '.png')\n",
    "                print('bye bye bitch')\n",
    "                continue\n",
    "            apel_net_sentiment =1 + (total_hawk - total_dovish)/(total_hawk + total_dovish)\n",
    "            new_net_sentiment = 100*(total_hawk - total_dovish)/(.5*N)\n",
    "            dict['apel_net_sentiment'] = apel_net_sentiment # type: ignore\n",
    "            dict['new_net_sentiment'] = new_net_sentiment\n",
    "            with open(file_name + '.json', 'w') as fp:\n",
    "                json.dump(dict, fp, indent=4)\n",
    "            \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uttkarsh/IP\n",
      "['/home/uttkarsh/IP/Governor Speeches/2004 Q2', '/home/uttkarsh/IP/Governor Speeches/2005 Q1', '/home/uttkarsh/IP/Governor Speeches/2008 Q3', '/home/uttkarsh/IP/Governor Speeches/2021 Q1', '/home/uttkarsh/IP/Governor Speeches/2018 Q1', '/home/uttkarsh/IP/Governor Speeches/2014 Q1', '/home/uttkarsh/IP/Governor Speeches/2019 Q3', '/home/uttkarsh/IP/Governor Speeches/2014 Q2', '/home/uttkarsh/IP/Governor Speeches/2017 Q4', '/home/uttkarsh/IP/Governor Speeches/2007 Q1', '/home/uttkarsh/IP/Governor Speeches/2011 Q3', '/home/uttkarsh/IP/Governor Speeches/2005 Q3', '/home/uttkarsh/IP/Governor Speeches/2009 Q4', '/home/uttkarsh/IP/Governor Speeches/2023 Q2', '/home/uttkarsh/IP/Governor Speeches/2011 Q2', '/home/uttkarsh/IP/Governor Speeches/2022 Q3', '/home/uttkarsh/IP/Governor Speeches/2011 Q1', '/home/uttkarsh/IP/Governor Speeches/2016 Q1', '/home/uttkarsh/IP/Governor Speeches/2023 Q1', '/home/uttkarsh/IP/Governor Speeches/2009 Q3', '/home/uttkarsh/IP/Governor Speeches/2016 Q2', '/home/uttkarsh/IP/Governor Speeches/2016 Q3', '/home/uttkarsh/IP/Governor Speeches/2022 Q1', '/home/uttkarsh/IP/Governor Speeches/2012 Q3', '/home/uttkarsh/IP/Governor Speeches/2009 Q2', '/home/uttkarsh/IP/Governor Speeches/2018 Q4', '/home/uttkarsh/IP/Governor Speeches/2005 Q4', '/home/uttkarsh/IP/Governor Speeches/2004 Q3', '/home/uttkarsh/IP/Governor Speeches/2005 Q2', '/home/uttkarsh/IP/Governor Speeches/2015 Q2', '/home/uttkarsh/IP/Governor Speeches/2007 Q4', '/home/uttkarsh/IP/Governor Speeches/2010 Q4', '/home/uttkarsh/IP/Governor Speeches/2021 Q3', '/home/uttkarsh/IP/Governor Speeches/2009 Q1', '/home/uttkarsh/IP/Governor Speeches/2012 Q1', '/home/uttkarsh/IP/Governor Speeches/2011 Q4', '/home/uttkarsh/IP/Governor Speeches/2007 Q2', '/home/uttkarsh/IP/Governor Speeches/2013 Q4', '/home/uttkarsh/IP/Governor Speeches/2010 Q3', '/home/uttkarsh/IP/Governor Speeches/2023 Q3', '/home/uttkarsh/IP/Governor Speeches/2010 Q2', '/home/uttkarsh/IP/Governor Speeches/2013 Q3', '/home/uttkarsh/IP/Governor Speeches/2020 Q1', '/home/uttkarsh/IP/Governor Speeches/2013 Q1', '/home/uttkarsh/IP/Governor Speeches/2019 Q2', '/home/uttkarsh/IP/Governor Speeches/2010 Q1', '/home/uttkarsh/IP/Governor Speeches/2020 Q3', '/home/uttkarsh/IP/Governor Speeches/2008 Q2', '/home/uttkarsh/IP/Governor Speeches/2008 Q1']\n",
      "/home/uttkarsh/IP/Governor Speeches/2004 Q2\n",
      "24 Apr 04 MP.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uttkarsh/IP/Governor Speeches/2005 Q1\n",
      "12 Feb 05 MP.pdf\n",
      "08 Jan 05 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2008 Q3\n",
      "4 July 08 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2021 Q1\n",
      "/home/uttkarsh/IP/Governor Speeches/2018 Q1\n",
      "/home/uttkarsh/IP/Governor Speeches/2014 Q1\n",
      "26 Feb 14 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2019 Q3\n",
      "/home/uttkarsh/IP/Governor Speeches/2014 Q2\n",
      "10 Apr 14 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2017 Q4\n",
      "27 Oct 17 MP.pdf\n",
      "16 Nov 17 MP.pdf\n",
      "20 Nov 17 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2007 Q1\n",
      "06 Mar 07 FM.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2011 Q3\n",
      "27 Sep 11 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2005 Q3\n",
      "3 Sep 05 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2009 Q4\n",
      "5 Oct 09 MP.pdf\n",
      "16 Oct 09 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2023 Q2\n",
      "27 Apr 23 FM.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2011 Q2\n",
      "17 Apr 11 FM.pdf\n",
      "23 June 11 MP.pdf\n",
      "10 May 11 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2022 Q3\n",
      "9 July 22 MP.pdf\n",
      "5 Sep 22 FM.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2011 Q1\n",
      "21 Feb 11 MP.pdf\n",
      "7 Jan 11 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2016 Q1\n",
      "12 Mar 16 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2023 Q1\n",
      "27 Jan 23 FM.pdf\n",
      "27 Jan 23 FSU.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2009 Q3\n",
      "11 Sep 09 MP.pdf\n",
      "31 July 09 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2016 Q2\n",
      "20 June 16 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2016 Q3\n",
      "26 July 16 MP.pdf\n",
      "26 Aug 16 FM.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2022 Q1\n",
      "4 Mar 22 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2012 Q3\n",
      "19 July 12 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2009 Q2\n",
      "25 Apr 09 MP.pdf\n",
      "29 June 09 MP.pdf\n",
      "22 May 09 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2018 Q4\n",
      "26 Oct 18 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2005 Q4\n",
      "4 Nov 05 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2004 Q3\n",
      "/home/uttkarsh/IP/Governor Speeches/2005 Q2\n",
      "/home/uttkarsh/IP/Governor Speeches/2015 Q2\n",
      "19 May 15 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2007 Q4\n",
      "/home/uttkarsh/IP/Governor Speeches/2010 Q4\n",
      "8 Dec 10 MP.pdf\n",
      "27 Oct 10 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2021 Q3\n",
      "16 July 21 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2009 Q1\n",
      "18 Feb 09 MP.pdf\n",
      "26 Mar 09 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2012 Q1\n",
      "1 Feb 12 MP.pdf\n",
      "22 Feb 12 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2011 Q4\n",
      "22 Nov 11 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2007 Q2\n",
      "9 Apr 07 FM.pdf\n",
      "28 May 07 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2013 Q4\n",
      "/home/uttkarsh/IP/Governor Speeches/2010 Q3\n",
      "27 Aug 10 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2023 Q3\n",
      "11 Aug 23.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2010 Q2\n",
      "27 Apr 10 MP.pdf\n",
      "25 Apr 10 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2013 Q3\n",
      "29 Aug 13 MP.pdf\n",
      "18 July 13 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2020 Q1\n",
      "24 Jan 20 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2013 Q1\n",
      "3 Jan 13 MP.pdf\n",
      "13 Mar 13 MP.pdf\n",
      "8 Mar 13 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2019 Q2\n",
      "12 Apr 19 MP.pdf\n",
      "29 June 19 FM.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2010 Q1\n",
      "24 Feb 10 MP.pdf\n",
      "4 Jan 10 MP.pdf\n",
      "12 Feb 10 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2020 Q3\n",
      "11 July 20 FSU.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2008 Q2\n",
      "5 June 08 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2008 Q1\n",
      "21 Jan 08 FM.pdf\n"
     ]
    }
   ],
   "source": [
    "make_wordcloud_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise_hd = {}\n",
    "def fn(df):\n",
    "    for date in df['Meeting_Date']:\n",
    "        dates = date.split('/')\n",
    "        file = dates[0] + '-' + dates[2] + '.pdf'\n",
    "        text = get_text(file)\n",
    "        token = filter(text)\n",
    "        bigram_list = get_bigram(token)\n",
    "        trigram_list = get_trigram(token)\n",
    "        N = len(token)\n",
    "        string_list = get_string_from_list(token)\n",
    "        cpi_hawkish = searchAndCount_single(bigram_list, 'cpi', ConsumerPricesInflation_Hawkish) + searchAndCount_double(bigram_list, 'cpi', 'inflation', ConsumerPricesInflation_Hawkish) + searchAndCount_triple(trigram_list, 'consumer', 'prices', 'inflation', ConsumerPricesInflation_Hawkish) + searchAndCount_single(bigram_list, 'inflation', ConsumerPricesInflation_Hawkish)\n",
    "        cpi_dovish = searchAndCount_single(bigram_list, 'cpi', ConsumerPricesInflation_Dovish) + searchAndCount_double(bigram_list, 'cpi', 'inflation', ConsumerPricesInflation_Dovish) + searchAndCount_triple(trigram_list, 'consumer', 'prices', 'inflation', ConsumerPricesInflation_Dovish) + searchAndCount_single(bigram_list, 'inflation', ConsumerPricesInflation_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        coreinflation_hawkish = searchAndCount_double(bigram_list, 'core', 'inflation', ConsumerPricesInflation_Hawkish)\n",
    "        coreinflation_dovish = searchAndCount_double(bigram_list, 'core', 'inflation', ConsumerPricesInflation_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        ip_inflation_hawkish = searchAndCount_double(bigram_list, 'inflation', 'pressure', InflationPressure_Hawkish)\n",
    "        ip_inflation_dovish = searchAndCount_double(bigram_list, 'inflation', 'pressure', InflationPressure_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        cs_hawkish = searchAndCount_double(bigram_list, 'consumer', 'spending', ConsumerSpending_Hawkish)\n",
    "        cs_dovish = searchAndCount_double(bigram_list, 'consumer', 'spending', ConsumerSpending_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        e_hawkish = searchAndCount_single(bigram_list, 'economic', EconomicActivity_Hawkish)\n",
    "        e_dovish = searchAndCount_single(bigram_list, 'economic', EconomicActivity_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        ea_hawkish = searchAndCount_double(bigram_list, 'economic', 'activity', EconomicActivity_Hawkish)\n",
    "        ea_dovish = searchAndCount_double(bigram_list, 'economic', 'activity', EconomicActivity_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        ru_hawkish = searchAndCount_double(bigram_list, 'resource', 'utilisation', ResourceUtilization_Hawkish) + searchAndCount_double(bigram_list, 'resource', 'utilization', ResourceUtilization_Hawkish)\n",
    "        ru_dovish = searchAndCount_double(bigram_list, 'resource', 'utilisation', ResourceUtilization_Dovish) + searchAndCount_double(bigram_list, 'resource', 'utilization', ResourceUtilization_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        em_hawkish = searchAndCount_single(bigram_list, 'employment', Employment_Hawkish)\n",
    "        em_dovish = searchAndCount_single(bigram_list, 'employment', Employment_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        lm_hawkish = searchAndCount_single(bigram_list, 'labour', LaborMarket_Hawkish) + searchAndCount_double(bigram_list, 'labour', 'market', LaborMarket_Hawkish)\n",
    "        lm_dovish = searchAndCount_single(bigram_list, 'labour', LaborMarket_Dovish) + searchAndCount_double(bigram_list, 'labour', 'market', LaborMarket_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        uem_hawkish = searchAndCount_single(bigram_list, 'unemployment', Unemployment_Hawkish)\n",
    "        uem_dovish = searchAndCount_single(bigram_list, 'unemployment', Unemployment_Dovish)\n",
    "\n",
    "        # make a dictionary of all above values and dump it in a json file\n",
    "        dict = {'cpi_hawkish': cpi_hawkish, 'cpi_dovish': cpi_dovish, 'coreinflation_hawkish': coreinflation_hawkish, 'coreinflation_dovish': coreinflation_dovish, 'ip_inflation_hawkish': ip_inflation_hawkish, 'ip_inflation_dovish': ip_inflation_dovish, 'cs_hawkish': cs_hawkish, 'cs_dovish': cs_dovish, 'e_hawkish': e_hawkish, 'e_dovish': e_dovish, 'ea_hawkish': ea_hawkish, 'ea_dovish': ea_dovish, 'ru_hawkish': ru_hawkish, 'ru_dovish': ru_dovish, 'em_hawkish': em_hawkish, 'em_dovish': em_dovish, 'lm_hawkish': lm_hawkish, 'lm_dovish': lm_dovish, 'uem_hawkish': uem_hawkish, 'uem_dovish': uem_dovish}\n",
    "        total_hawk = cpi_hawkish + coreinflation_hawkish + ip_inflation_hawkish + cs_hawkish + e_hawkish + ea_hawkish + ru_hawkish + em_hawkish + lm_hawkish + uem_hawkish\n",
    "        total_dovish = cpi_dovish + coreinflation_dovish + ip_inflation_dovish + cs_dovish + e_dovish + ea_dovish + ru_dovish + em_dovish + lm_dovish + uem_dovish\n",
    "        if(total_hawk + total_dovish == 0):\n",
    "            print('bhai is file ne to kaat dia')\n",
    "            continue\n",
    "        apel_net_sentiment =1 + (total_hawk - total_dovish)/(total_hawk + total_dovish)\n",
    "        # new_net_sentiment = 100*(total_hawk - total_dovish)/(.5*N)\n",
    "        surprise_hd[date] = {}\n",
    "        surprise_hd[date]['hd'] = (apel_net_sentiment)\n",
    "        surprise_hd[date]['surprise'] = (df[df['Meeting_Date'] == date]['MP_Surprise'].values[0])\n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP\n",
      "53\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2004 Q1\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2004 Q2\n",
      "24 Apr 04 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2004 Q4\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2005 Q1\n",
      "08 Jan 05 MP.json\n",
      "12 Feb 05 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2005 Q3\n",
      "3 Sep 05 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2005 Q4\n",
      "4 Nov 05 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2006 Q1\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2006 Q2\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2006 Q3\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2006 Q4\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2007 Q1\n",
      "06 Mar 07 FM.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2007 Q2\n",
      "28 May 07 MP.json\n",
      "9 Apr 07 FM.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2007 Q3\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2008 Q1\n",
      "21 Jan 08 FM.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2008 Q2\n",
      "5 June 08 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2008 Q3\n",
      "4 July 08 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2008 Q4\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2009 Q1\n",
      "18 Feb 09 MP.json\n",
      "26 Mar 09 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2009 Q2\n",
      "22 May 09 MP.json\n",
      "25 Apr 09 MP.json\n",
      "29 June 09 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2009 Q3\n",
      "11 Sep 09 MP.json\n",
      "31 July 09 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2009 Q4\n",
      "16 Oct 09 MP.json\n",
      "5 Oct 09 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2010 Q1\n",
      "12 Feb 10 MP.json\n",
      "24 Feb 10 MP.json\n",
      "4 Jan 10 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2010 Q2\n",
      "25 Apr 10 MP.json\n",
      "27 Apr 10 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2010 Q3\n",
      "27 Aug 10 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2010 Q4\n",
      "27 Oct 10 MP.json\n",
      "8 Dec 10 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2011 Q1\n",
      "21 Feb 11 MP.json\n",
      "7 Jan 11 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2011 Q2\n",
      "10 May 11 MP.json\n",
      "17 Apr 11 FM.json\n",
      "23 June 11 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2011 Q3\n",
      "27 Sep 11 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2011 Q4\n",
      "22 Nov 11 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2012 Q1\n",
      "1 Feb 12 MP.json\n",
      "22 Feb 12 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2012 Q2\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2012 Q3\n",
      "19 July 12 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2012 Q4\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2013 Q1\n",
      "13 Mar 13 MP.json\n",
      "3 Jan 13 MP.json\n",
      "8 Mar 13 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2013 Q2\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2013 Q3\n",
      "18 July 13 MP.json\n",
      "29 Aug 13 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2014 Q1\n",
      "26 Feb 14 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2014 Q2\n",
      "10 Apr 14 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2015 Q2\n",
      "19 May 15 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2016 Q1\n",
      "12 Mar 16 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2016 Q2\n",
      "20 June 16 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2016 Q3\n",
      "26 Aug 16 FM.json\n",
      "26 July 16 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2017 Q4\n",
      "16 Nov 17 MP.json\n",
      "20 Nov 17 MP.json\n",
      "27 Oct 17 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2018 Q4\n",
      "26 Oct 18 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2019 Q2\n",
      "12 Apr 19 MP.json\n",
      "29 June 19 FM.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2020 Q1\n",
      "24 Jan 20 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2020 Q3\n",
      "11 July 20 FSU.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2021 Q3\n",
      "16 July 21 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2022 Q1\n",
      "4 Mar 22 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2022 Q3\n",
      "5 Sep 22 FM.json\n",
      "9 July 22 MP.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2023 Q1\n",
      "27 Jan 23 FM.json\n",
      "27 Jan 23 FSU.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2023 Q2\n",
      "27 Apr 23 FM.json\n",
      "C:\\Users\\karti\\OneDrive\\Desktop\\IP\\IP/Governor Speeches\\2023 Q3\n",
      "11 Aug 23.json\n",
      "{'24 Apr 04 MP': {'apel_net_sentiment': 1.6363636363636362, 'new_net_sentiment': 1.3307984790874525}, '08 Jan 05 MP': {'apel_net_sentiment': 0.0, 'new_net_sentiment': -2.3210831721470018}, '12 Feb 05 MP': {'apel_net_sentiment': 2.0, 'new_net_sentiment': 0.4579278763594734}, '3 Sep 05 MP': {'apel_net_sentiment': 2.0, 'new_net_sentiment': 0.4807692307692308}, '4 Nov 05 MP': {'apel_net_sentiment': 0.6153846153846154, 'new_net_sentiment': -1.1406844106463878}, '06 Mar 07 FM': {'apel_net_sentiment': 0.6666666666666667, 'new_net_sentiment': -0.17965416573096787}, '28 May 07 MP': {'apel_net_sentiment': 1.5, 'new_net_sentiment': 0.7155635062611807}, '9 Apr 07 FM': {'apel_net_sentiment': 2.0, 'new_net_sentiment': 0.4929143561306223}, '21 Jan 08 FM': {'apel_net_sentiment': 0.2962962962962963, 'new_net_sentiment': -1.280323450134771}, '5 June 08 MP': {'apel_net_sentiment': 2.0, 'new_net_sentiment': 0.819252432155658}, '4 July 08 MP': {'apel_net_sentiment': 1.6296296296296298, 'new_net_sentiment': 1.046475838719606}, '18 Feb 09 MP': {'apel_net_sentiment': 0.0, 'new_net_sentiment': -3.5921817221341787}, '26 Mar 09 MP': {'apel_net_sentiment': 0.0, 'new_net_sentiment': -3.7449392712550607}, '22 May 09 MP': {'apel_net_sentiment': 0.8421052631578947, 'new_net_sentiment': -0.32679738562091504}, '25 Apr 09 MP': {'apel_net_sentiment': 0.8059701492537313, 'new_net_sentiment': -1.3032581453634084}, '29 June 09 MP': {'apel_net_sentiment': 0.0, 'new_net_sentiment': -0.22026431718061673}, '11 Sep 09 MP': {'apel_net_sentiment': 0.0, 'new_net_sentiment': -1.049618320610687}, '31 July 09 MP': {'apel_net_sentiment': 0.5333333333333333, 'new_net_sentiment': -0.9150326797385621}, '16 Oct 09 MP': {'apel_net_sentiment': 0.9333333333333333, 'new_net_sentiment': -0.08745080891998251}, '5 Oct 09 MP': {'apel_net_sentiment': 0.9375, 'new_net_sentiment': -0.5826656955571741}, '12 Feb 10 MP': {'apel_net_sentiment': 1.5, 'new_net_sentiment': 0.6902502157031924}, '24 Feb 10 MP': {'apel_net_sentiment': 1.0, 'new_net_sentiment': 0.0}, '4 Jan 10 MP': {'apel_net_sentiment': 2.0, 'new_net_sentiment': 0.7494145199063232}, '25 Apr 10 MP': {'apel_net_sentiment': 0.7755102040816326, 'new_net_sentiment': -0.909466721785862}, '27 Apr 10 MP': {'apel_net_sentiment': 0.0, 'new_net_sentiment': -0.3270645952575634}, '27 Aug 10 MP': {'apel_net_sentiment': 0.5, 'new_net_sentiment': -0.5732712289501971}, '27 Oct 10 MP': {'apel_net_sentiment': 0.0, 'new_net_sentiment': -0.7590132827324478}, '8 Dec 10 MP': {'apel_net_sentiment': 2.0, 'new_net_sentiment': 1.2448132780082988}, '21 Feb 11 MP': {'apel_net_sentiment': 2.0, 'new_net_sentiment': 0.5025125628140703}, '7 Jan 11 MP': {'apel_net_sentiment': 0.0, 'new_net_sentiment': -0.5034612964128382}, '10 May 11 MP': {'apel_net_sentiment': 1.7777777777777777, 'new_net_sentiment': 1.0144927536231885}, '17 Apr 11 FM': {'apel_net_sentiment': 0.9545454545454546, 'new_net_sentiment': -0.2517306482064191}, '23 June 11 MP': {'apel_net_sentiment': 0.0, 'new_net_sentiment': -1.0059729644765798}, '27 Sep 11 MP': {'apel_net_sentiment': 0.7787610619469026, 'new_net_sentiment': -1.71939477303989}, '22 Nov 11 MP': {'apel_net_sentiment': 1.5757575757575757, 'new_net_sentiment': 3.8630972551677396}, '1 Feb 12 MP': {'apel_net_sentiment': 0.6956521739130435, 'new_net_sentiment': -1.1155378486055776}, '22 Feb 12 MP': {'apel_net_sentiment': 0.0, 'new_net_sentiment': -1.2593467138921683}, '19 July 12 MP': {'apel_net_sentiment': 0.0, 'new_net_sentiment': -1.8495684340320593}, '13 Mar 13 MP': {'apel_net_sentiment': 0.8068181818181819, 'new_net_sentiment': -2.1642266072565244}, '3 Jan 13 MP': {'apel_net_sentiment': 1.0, 'new_net_sentiment': 0.0}, '8 Mar 13 MP': {'apel_net_sentiment': 1.1908396946564885, 'new_net_sentiment': 1.9357336430507162}, '18 July 13 MP': {'apel_net_sentiment': 0.24242424242424243, 'new_net_sentiment': -1.722356183258698}, '29 Aug 13 MP': {'apel_net_sentiment': 0.8979591836734694, 'new_net_sentiment': -0.33123550844650546}, '26 Feb 14 MP': {'apel_net_sentiment': 0.8136882129277566, 'new_net_sentiment': -4.967055245818551}, '10 Apr 14 MP': {'apel_net_sentiment': 0.0, 'new_net_sentiment': -0.7380073800738007}, '19 May 15 MP': {'apel_net_sentiment': 0.43137254901960786, 'new_net_sentiment': -1.5263157894736843}, '12 Mar 16 MP': {'apel_net_sentiment': 0.07407407407407407, 'new_net_sentiment': -2.239140170174653}, '20 June 16 MP': {'apel_net_sentiment': 0.9622641509433962, 'new_net_sentiment': -0.9991673605328892}, '26 Aug 16 FM': {'apel_net_sentiment': 0.4897959183673469, 'new_net_sentiment': -2.7027027027027026}, '26 July 16 MP': {'apel_net_sentiment': 1.053191489361702, 'new_net_sentiment': 1.2422360248447204}, '16 Nov 17 MP': {'apel_net_sentiment': 0.0, 'new_net_sentiment': -0.42283298097251587}, '20 Nov 17 MP': {'apel_net_sentiment': 0.8712871287128713, 'new_net_sentiment': -0.8499509643674403}, '27 Oct 17 MP': {'apel_net_sentiment': 0.8712871287128713, 'new_net_sentiment': -0.8499509643674403}, '26 Oct 18 MP': {'apel_net_sentiment': 1.8064516129032258, 'new_net_sentiment': 1.2478163214374844}, '12 Apr 19 MP': {'apel_net_sentiment': 1.0, 'new_net_sentiment': 0.0}, '29 June 19 FM': {'apel_net_sentiment': 0.7, 'new_net_sentiment': -0.4429678848283499}, '24 Jan 20 MP': {'apel_net_sentiment': 1.0769230769230769, 'new_net_sentiment': 0.4553734061930783}, '11 July 20 FSU': {'apel_net_sentiment': 2.0, 'new_net_sentiment': 0.6393180607352158}, '16 July 21 MP': {'apel_net_sentiment': 0.9302325581395349, 'new_net_sentiment': -0.547945205479452}, '4 Mar 22 MP': {'apel_net_sentiment': 0.8484848484848485, 'new_net_sentiment': -0.8343763037129746}, '5 Sep 22 FM': {'apel_net_sentiment': 0.0, 'new_net_sentiment': -1.8252933507170794}, '9 July 22 MP': {'apel_net_sentiment': 1.2323232323232323, 'new_net_sentiment': 4.222120238641579}, '27 Jan 23 FM': {'apel_net_sentiment': 1.6800000000000002, 'new_net_sentiment': 3.4943473792394655}, '27 Jan 23 FSU': {'apel_net_sentiment': 1.6800000000000002, 'new_net_sentiment': 3.4943473792394655}, '27 Apr 23 FM': {'apel_net_sentiment': 2.0, 'new_net_sentiment': 0.4681100058513751}, '11 Aug 23': {'apel_net_sentiment': 0.6666666666666667, 'new_net_sentiment': -0.8359456635318704}}\n",
      "              apel_net_sentiment  new_net_sentiment\n",
      "24 Apr 04 MP            1.636364           1.330798\n",
      "08 Jan 05 MP            0.000000          -2.321083\n",
      "12 Feb 05 MP            2.000000           0.457928\n",
      "3 Sep 05 MP             2.000000           0.480769\n",
      "4 Nov 05 MP             0.615385          -1.140684\n"
     ]
    }
   ],
   "source": [
    "# access each json file in each folder of the Governor Speeches folder\n",
    "\n",
    "surprise_hd_temp = {}\n",
    "os.chdir('C:/Users/karti/OneDrive/Desktop/IP/IP')\n",
    "\n",
    "def get_folder_names():\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    folder_names = glob.glob(cwd + '/Governor Speeches/*')\n",
    "    return folder_names\n",
    "\n",
    "folder_names = get_folder_names()\n",
    "print(len(folder_names))\n",
    "\n",
    "for i in range(len(folder_names)):\n",
    "    os.chdir(folder_names[i])\n",
    "    print(folder_names[i])\n",
    "    file_names = glob.glob('*.json')\n",
    "    for file_name in file_names:\n",
    "        print(file_name)\n",
    "        # open each json file and get the date and the net sentiment and store them in a csv file\n",
    "        with open(file_name) as f:\n",
    "            data = json.load(f)\n",
    "        date = file_name[:-5]\n",
    "        surprise_hd_temp[date] = {}\n",
    "        surprise_hd_temp[date]['apel_net_sentiment'] = data['apel_net_sentiment'] \n",
    "        surprise_hd_temp[date]['new_net_sentiment'] = data['new_net_sentiment']\n",
    "print(surprise_hd_temp)\n",
    "\n",
    "df = pd.DataFrame.from_dict(surprise_hd_temp, orient='index')\n",
    "df.to_csv('hd_data.csv')\n",
    "print(df.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
