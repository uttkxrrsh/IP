{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import regex as re\n",
    "from nltk import word_tokenize \n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from dictionary import *\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "# filename is test.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uttkarsh/IP\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/uttkarsh/IP/')\n",
    "print(os.getcwd())\n",
    "def get_text(file):\n",
    "    text = ''\n",
    "    with open(file, 'rb') as pdfFileObj:\n",
    "        pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "        num_pages = len(pdfReader.pages)\n",
    "        for i in range(num_pages):\n",
    "            pageObj = pdfReader.pages[i]\n",
    "            text += pageObj.extract_text()\n",
    "        text = text.replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "def get_string_from_list(list):\n",
    "    string = ''\n",
    "    for i in range(len(list)):\n",
    "        string += list[i]\n",
    "        if i != len(list)-1:\n",
    "            string += ' '\n",
    "    return string\n",
    "\n",
    "def filter(text):\n",
    "    token = word_tokenize(text)\n",
    "    token = [word.lower() for word in token]\n",
    "    token = [word for word in token if word.isalpha()]\n",
    "    token = [word for word in token if (word == 'down' or not word in stop_words )]\n",
    "    # remoev single character\n",
    "    token = [word for word in token if len(word) > 1]\n",
    "    # remove th\n",
    "    token = [word for word in token if word != 'th']\n",
    "    for i in range(len(token)):\n",
    "        if token[i] == 'per' and token[i+1] == 'cent':\n",
    "            token[i] = 'percent'\n",
    "            token[i+1] = ''\n",
    "    token = [word for word in token if word != '']\n",
    "    return token\n",
    "\n",
    "def get_bigram(token):\n",
    "    token = [stemmer.stem(word) for word in token]\n",
    "    bigram = ngrams(token, 2)\n",
    "    return list(bigram)\n",
    "\n",
    "def get_trigram(token):\n",
    "    token = [stemmer.stem(word) for word in token]\n",
    "    trigram = ngrams(token, 3)\n",
    "    return list(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchAndCount_single(bigram_list, term, hdlist):\n",
    "    term = stemmer.stem(term)\n",
    "    count = 0\n",
    "    for i in range(len(bigram_list)):\n",
    "        if term in bigram_list[i]:\n",
    "            for j in range(len(hdlist)):\n",
    "                if i-7 < 0:\n",
    "                    for k in range(i+7):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "                elif i+7 > len(bigram_list):\n",
    "                    for k in range(i, len(bigram_list)):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "                else:\n",
    "                    for k in range(i-7, i+7):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def searchAndCount_double(bigram_list, term1, term2, hdlist):\n",
    "    term1 = stemmer.stem(term1)\n",
    "    term2 = stemmer.stem(term2)\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(bigram_list)):\n",
    "        if term1 in bigram_list[i] and term2 in bigram_list[i]:\n",
    "            for j in range(len(hdlist)):\n",
    "                if i-7 < 0:\n",
    "                    for k in range(i+7):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "                elif i+7 > len(bigram_list):\n",
    "                    for k in range(i, len(bigram_list)):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "                else:\n",
    "                    for k in range(i-7, i+7):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "    return count\n",
    "\n",
    "def searchAndCount_triple(trigram_list, term1, term2, term3, hdlist):\n",
    "    term1 = stemmer.stem(term1)\n",
    "    term2 = stemmer.stem(term2)\n",
    "    term3 = stemmer.stem(term3)\n",
    "    count = 0\n",
    "    for i in range(len(trigram_list)):\n",
    "        if term1 in trigram_list[i] and term2 in trigram_list[i] and term3 in trigram_list[i]:\n",
    "            for j in range(len(hdlist)):\n",
    "                if i-7 < 0:\n",
    "                    for k in range(i+7):\n",
    "                        if hdlist[j] in trigram_list[k]:\n",
    "                            count += 1\n",
    "                elif i+7 > len(trigram_list):\n",
    "                    for k in range(i, len(trigram_list)):\n",
    "                        if hdlist[j] in trigram_list[k]:\n",
    "                            count += 1\n",
    "                else:\n",
    "                    for k in range(i-7, i+7):\n",
    "                        if hdlist[j] in trigram_list[k]:\n",
    "                            count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "100\n",
      "55\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "cpi_hawkish = searchAndCount_triple(trigram_list, \"consumer\", \"prices\", \"inflation\", ConsumerPricesInflation_Hawkish)\n",
    "print(cpi_hawkish)\n",
    "cpi_dovish = searchAndCount_triple(trigram_list,  \"consumer\", \"prices\", \"inflation\", ConsumerPricesInflation_Dovish)\n",
    "print(cpi_dovish)\n",
    "\n",
    "ip_hawkish = searchAndCount_single(bigram_list, \"inflation\", InflationPressure_Hawkish)\n",
    "print(ip_hawkish)\n",
    "ip_dovish = searchAndCount_single(bigram_list, \"inflation\", InflationPressure_Dovish)\n",
    "print(ip_dovish)\n",
    "\n",
    "cs_hawkish = searchAndCount_double(bigram_list, \"consumer\", \"spending\", ConsumerSpending_Hawkish)\n",
    "print(cs_hawkish)\n",
    "cs_dovish = searchAndCount_double(bigram_list, \"consumer\", \"spending\", ConsumerSpending_Dovish)\n",
    "print(cs_dovish)\n",
    "\n",
    "ea_hawkish = searchAndCount_single(bigram_list, \"econom\", EconomicActivity_Hawkish)\n",
    "print(ea_hawkish)\n",
    "ea_dovish = searchAndCount_single(bigram_list, \"economic\", EconomicActivity_Dovish)\n",
    "print(ea_dovish)\n",
    "\n",
    "ru_hawkish = searchAndCount_single(bigram_list, \"resource\", ResourceUtilization_Hawkish)\n",
    "print(ru_hawkish)\n",
    "ru_dovish = searchAndCount_single(bigram_list, \"resource\", ResourceUtilization_Dovish)\n",
    "print(ru_dovish)\n",
    "\n",
    "e_hawkish = searchAndCount_single(bigram_list, \"employment\", Employment_Hawkish)\n",
    "print(e_hawkish)\n",
    "e_dovish = searchAndCount_single(bigram_list, \"employment\", Employment_Dovish)\n",
    "print(e_dovish)\n",
    "\n",
    "lm_hawkish = searchAndCount_single(bigram_list, \"labour\", LaborMarket_Hawkish)\n",
    "print(lm_hawkish)\n",
    "lm_dovish = searchAndCount_single(bigram_list, \"labour\", LaborMarket_Dovish)\n",
    "print(lm_dovish)\n",
    "\n",
    "ue_hawkish = searchAndCount_single(bigram_list, \"unemployment\", Unemployment_Hawkish)\n",
    "print(ue_hawkish)\n",
    "ue_dovish = searchAndCount_single(bigram_list, \"unemployment\", Unemployment_Dovish)\n",
    "print(ue_dovish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.282051282051282\n"
     ]
    }
   ],
   "source": [
    "# Calculating net hawkishness\n",
    "\n",
    "hawk = cpi_hawkish + ip_hawkish + cs_hawkish + ea_hawkish + ru_hawkish + e_hawkish + lm_hawkish + ue_hawkish\n",
    "dove = cpi_dovish + ip_dovish + cs_dovish + ea_dovish + ru_dovish + e_dovish + lm_dovish + ue_dovish\n",
    "\n",
    "net_hawkishness = 1 + ((hawk - dove) / (hawk + dove))\n",
    "\n",
    "print(net_hawkishness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = searchAndCount_single(bigram_list, \"cpi\", ConsumerPricesInflation_Hawkish)\n",
    "test2 = searchAndCount_double(bigram_list, \"cpi\", \"inflation\", ConsumerPricesInflation_Dovish)\n",
    "test3 = searchAndCount_triple(trigram_list, \"consumer\", \"prices\", \"inflation\", ConsumerPricesInflation_Hawkish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 5 0\n"
     ]
    }
   ],
   "source": [
    "print(test1, test2, test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is  a file named 'Governer Speeches' which contain all the folders with speeches in them. I want to open the files on by one and run the code on them.\n",
    "# after running the code I want to make dictionary of the net hawkishness of each speech and the date of the speech. and save it as a json file with the same name as the file name i run the code\n",
    "# go to the folder 'Governer Speeches' and for all the subfolders in it open eaach pdf file in it and run the code\n",
    "\n",
    "# opening each folder\n",
    "os.chdir('/home/uttkarsh/IP')\n",
    "\n",
    "def get_folder_names():\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    folder_names = glob.glob(cwd + '/Governor Speeches/*')\n",
    "    return folder_names\n",
    "\n",
    "# opening each file in each folder\n",
    "def make_wordcloud_image():\n",
    "    folder_names = get_folder_names()\n",
    "    print(folder_names)\n",
    "    # show this as tqdm\n",
    "    for i in tqdm(range(len(folder_names))):\n",
    "        os.chdir(folder_names[i])\n",
    "        file_names = glob.glob('*.pdf')\n",
    "        for file_name in file_names:\n",
    "            text = get_text(file_name)\n",
    "            file_name = file_name[:-4] # removing .pdf from the file name\n",
    "            if os.path.exists(file_name + '_trigram' + 'json'):\n",
    "                os.remove(file_name + '_trigram' + 'json')\n",
    "            if os.path.exists(file_name + 'json'):\n",
    "                os.remove(file_name + 'json')\n",
    "            if os.path.exists(file_name + '.json'):\n",
    "                os.remove(file_name + '.json')\n",
    "            token = filter(text)\n",
    "            string_list = get_string_from_list(token)\n",
    "            bigram_list = get_bigram(token)\n",
    "            trigram_list = get_trigram(token)\n",
    "            wordcloud = WordCloud(width=1600, height=800, max_font_size=200).generate(string_list)\n",
    "            wordcloud.to_file(file_name + '.png')\n",
    "            with open(file_name + '_bigram' + '.json', 'w') as fp:\n",
    "                json.dump(bigram_list, fp, indent=4)\n",
    "            with open(file_name + '_trigram' + '.json', 'w') as fp:\n",
    "                json.dump(trigram_list, fp, indent=4)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uttkarsh/IP\n",
      "['/home/uttkarsh/IP/Governor Speeches/2021 Q1', '/home/uttkarsh/IP/Governor Speeches/2018 Q1', '/home/uttkarsh/IP/Governor Speeches/2014 Q1', '/home/uttkarsh/IP/Governor Speeches/2019 Q3', '/home/uttkarsh/IP/Governor Speeches/2014 Q2', '/home/uttkarsh/IP/Governor Speeches/2017 Q4', '/home/uttkarsh/IP/Governor Speeches/2023 Q2', '/home/uttkarsh/IP/Governor Speeches/2022 Q3', '/home/uttkarsh/IP/Governor Speeches/2016 Q1', '/home/uttkarsh/IP/Governor Speeches/2023 Q1', '/home/uttkarsh/IP/Governor Speeches/2016 Q2', '/home/uttkarsh/IP/Governor Speeches/2016 Q3', '/home/uttkarsh/IP/Governor Speeches/2022 Q1', '/home/uttkarsh/IP/Governor Speeches/2018 Q4', '/home/uttkarsh/IP/Governor Speeches/2015 Q2', '/home/uttkarsh/IP/Governor Speeches/2021 Q3', '/home/uttkarsh/IP/Governor Speeches/2023 Q3', '/home/uttkarsh/IP/Governor Speeches/2020 Q1', '/home/uttkarsh/IP/Governor Speeches/2019 Q2', '/home/uttkarsh/IP/Governor Speeches/2020 Q3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:27<00:00,  4.37s/it]\n"
     ]
    }
   ],
   "source": [
    "make_wordcloud_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
