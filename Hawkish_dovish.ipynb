{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import regex as re\n",
    "from nltk import word_tokenize \n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from dictionary import *\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "# filename is test.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uttkarsh/IP\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/uttkarsh/IP/')\n",
    "print(os.getcwd())\n",
    "def get_text(file):\n",
    "    text = ''\n",
    "    with open(file, 'rb') as pdfFileObj:\n",
    "        pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "        num_pages = len(pdfReader.pages)\n",
    "        for i in range(num_pages):\n",
    "            pageObj = pdfReader.pages[i]\n",
    "            text += pageObj.extract_text()\n",
    "        text = text.replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "def get_string_from_list(list):\n",
    "    string = ''\n",
    "    for i in range(len(list)):\n",
    "        string += list[i]\n",
    "        if i != len(list)-1:\n",
    "            string += ' '\n",
    "    return string\n",
    "\n",
    "def filter(text):\n",
    "    token = word_tokenize(text)\n",
    "    token = [word.lower() for word in token]\n",
    "    token = [word for word in token if word.isalpha()]\n",
    "    token = [word for word in token if (word == 'down' or not word in stop_words )]\n",
    "    # remoev single character\n",
    "    token = [word for word in token if len(word) > 1]\n",
    "    # remove th\n",
    "    token = [word for word in token if word != 'th']\n",
    "    for i in range(len(token)):\n",
    "        if token[i] == 'per' and token[i+1] == 'cent':\n",
    "            token[i] = 'percent'\n",
    "            token[i+1] = ''\n",
    "    token = [word for word in token if word != '']\n",
    "    return token\n",
    "\n",
    "def get_bigram(token):\n",
    "    token = [stemmer.stem(word) for word in token]\n",
    "    bigram = ngrams(token, 2)\n",
    "    return list(bigram)\n",
    "\n",
    "def get_trigram(token):\n",
    "    token = [stemmer.stem(word) for word in token]\n",
    "    trigram = ngrams(token, 3)\n",
    "    return list(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchAndCount_single(bigram_list, term, hdlist):\n",
    "    term = stemmer.stem(term)\n",
    "    count = 0\n",
    "    for i in range(len(bigram_list)):\n",
    "        if term in bigram_list[i]:\n",
    "            for j in range(len(hdlist)):\n",
    "                if i-7 < 0:\n",
    "                    for k in range(i+7):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "                elif i+7 > len(bigram_list):\n",
    "                    for k in range(i, len(bigram_list)):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "                else:\n",
    "                    for k in range(i-7, i+7):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def searchAndCount_double(bigram_list, term1, term2, hdlist):\n",
    "    term1 = stemmer.stem(term1)\n",
    "    term2 = stemmer.stem(term2)\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(bigram_list)):\n",
    "        if term1 in bigram_list[i] and term2 in bigram_list[i]:\n",
    "            for j in range(len(hdlist)):\n",
    "                if i-7 < 0:\n",
    "                    for k in range(i+7):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "                elif i+7 > len(bigram_list):\n",
    "                    for k in range(i, len(bigram_list)):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "                else:\n",
    "                    for k in range(i-7, i+7):\n",
    "                        if hdlist[j] in bigram_list[k]:\n",
    "                            count += 1\n",
    "    return count\n",
    "\n",
    "def searchAndCount_triple(trigram_list, term1, term2, term3, hdlist):\n",
    "    term1 = stemmer.stem(term1)\n",
    "    term2 = stemmer.stem(term2)\n",
    "    term3 = stemmer.stem(term3)\n",
    "    count = 0\n",
    "    for i in range(len(trigram_list)):\n",
    "        if term1 in trigram_list[i] and term2 in trigram_list[i] and term3 in trigram_list[i]:\n",
    "            for j in range(len(hdlist)):\n",
    "                if i-7 < 0:\n",
    "                    for k in range(i+7):\n",
    "                        if hdlist[j] in trigram_list[k]:\n",
    "                            count += 1\n",
    "                elif i+7 > len(trigram_list):\n",
    "                    for k in range(i, len(trigram_list)):\n",
    "                        if hdlist[j] in trigram_list[k]:\n",
    "                            count += 1\n",
    "                else:\n",
    "                    for k in range(i-7, i+7):\n",
    "                        if hdlist[j] in trigram_list[k]:\n",
    "                            count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is  a file named 'Governer Speeches' which contain all the folders with speeches in them. I want to open the files on by one and run the code on them.\n",
    "# after running the code I want to make dictionary of the net hawkishness of each speech and the date of the speech. and save it as a json file with the same name as the file name i run the code\n",
    "# go to the folder 'Governer Speeches' and for all the subfolders in it open eaach pdf file in it and run the code\n",
    "\n",
    "# opening each folder\n",
    "os.chdir('/home/uttkarsh/IP')\n",
    "\n",
    "def get_folder_names():\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    folder_names = glob.glob(cwd + '/Governor Speeches/*')\n",
    "    return folder_names\n",
    "\n",
    "# opening each file in each folder\n",
    "def make_wordcloud_image():\n",
    "    folder_names = get_folder_names()\n",
    "    print(folder_names)\n",
    "    # show this as tqdm\n",
    "    for i in range(len(folder_names)):\n",
    "        print(folder_names[i])\n",
    "        os.chdir(folder_names[i])\n",
    "        file_names = glob.glob('*.pdf')\n",
    "        for file_name in file_names:\n",
    "            print(file_name)\n",
    "            text = get_text(file_name)\n",
    "            file_name = file_name[:-4] # removing .pdf from the file name\n",
    "            if os.path.exists(file_name + '_trigram' + '.json'):\n",
    "                os.remove(file_name + '_trigram' + '.json')\n",
    "            if os.path.exists(file_name + '_bigram' + '.json'):\n",
    "                os.remove(file_name + '_bigram' + '.json')\n",
    "            token = filter(text)\n",
    "            N = len(token)\n",
    "            string_list = get_string_from_list(token)\n",
    "            bigram_list = get_bigram(token)\n",
    "            trigram_list = get_trigram(token)\n",
    "            wordcloud = WordCloud(width=1600, height=800, max_font_size=200).generate(string_list)\n",
    "            wordcloud.to_file(file_name + '.png')\n",
    "\n",
    "            cpi_hawkish = searchAndCount_single(bigram_list, 'cpi', ConsumerPricesInflation_Hawkish) + searchAndCount_double(bigram_list, 'cpi', 'inflation', ConsumerPricesInflation_Hawkish) + searchAndCount_triple(trigram_list, 'consumer', 'prices', 'inflation', ConsumerPricesInflation_Hawkish) + searchAndCount_single(bigram_list, 'inflation', ConsumerPricesInflation_Hawkish)\n",
    "            cpi_dovish = searchAndCount_single(bigram_list, 'cpi', ConsumerPricesInflation_Dovish) + searchAndCount_double(bigram_list, 'cpi', 'inflation', ConsumerPricesInflation_Dovish) + searchAndCount_triple(trigram_list, 'consumer', 'prices', 'inflation', ConsumerPricesInflation_Dovish) + searchAndCount_single(bigram_list, 'inflation', ConsumerPricesInflation_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            coreinflation_hawkish = searchAndCount_double(bigram_list, 'core', 'inflation', ConsumerPricesInflation_Hawkish)\n",
    "            coreinflation_dovish = searchAndCount_double(bigram_list, 'core', 'inflation', ConsumerPricesInflation_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            ip_inflation_hawkish = searchAndCount_double(bigram_list, 'inflation', 'pressure', InflationPressure_Hawkish)\n",
    "            ip_inflation_dovish = searchAndCount_double(bigram_list, 'inflation', 'pressure', InflationPressure_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            cs_hawkish = searchAndCount_double(bigram_list, 'consumer', 'spending', ConsumerSpending_Hawkish)\n",
    "            cs_dovish = searchAndCount_double(bigram_list, 'consumer', 'spending', ConsumerSpending_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            e_hawkish = searchAndCount_single(bigram_list, 'economic', EconomicActivity_Hawkish)\n",
    "            e_dovish = searchAndCount_single(bigram_list, 'economic', EconomicActivity_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            ea_hawkish = searchAndCount_double(bigram_list, 'economic', 'activity', EconomicActivity_Hawkish)\n",
    "            ea_dovish = searchAndCount_double(bigram_list, 'economic', 'activity', EconomicActivity_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            ru_hawkish = searchAndCount_double(bigram_list, 'resource', 'utilisation', ResourceUtilization_Hawkish) + searchAndCount_double(bigram_list, 'resource', 'utilization', ResourceUtilization_Hawkish)\n",
    "            ru_dovish = searchAndCount_double(bigram_list, 'resource', 'utilisation', ResourceUtilization_Dovish) + searchAndCount_double(bigram_list, 'resource', 'utilization', ResourceUtilization_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            em_hawkish = searchAndCount_single(bigram_list, 'employment', Employment_Hawkish)\n",
    "            em_dovish = searchAndCount_single(bigram_list, 'employment', Employment_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            lm_hawkish = searchAndCount_single(bigram_list, 'labour', LaborMarket_Hawkish) + searchAndCount_double(bigram_list, 'labour', 'market', LaborMarket_Hawkish)\n",
    "            lm_dovish = searchAndCount_single(bigram_list, 'labour', LaborMarket_Dovish) + searchAndCount_double(bigram_list, 'labour', 'market', LaborMarket_Dovish)\n",
    "            # --------------------------------------------------------------------------------------------\n",
    "            uem_hawkish = searchAndCount_single(bigram_list, 'unemployment', Unemployment_Hawkish)\n",
    "            uem_dovish = searchAndCount_single(bigram_list, 'unemployment', Unemployment_Dovish)\n",
    "\n",
    "            # make a dictionary of all above values and dump it in a json file\n",
    "            dict = {'cpi_hawkish': cpi_hawkish, 'cpi_dovish': cpi_dovish, 'coreinflation_hawkish': coreinflation_hawkish, 'coreinflation_dovish': coreinflation_dovish, 'ip_inflation_hawkish': ip_inflation_hawkish, 'ip_inflation_dovish': ip_inflation_dovish, 'cs_hawkish': cs_hawkish, 'cs_dovish': cs_dovish, 'e_hawkish': e_hawkish, 'e_dovish': e_dovish, 'ea_hawkish': ea_hawkish, 'ea_dovish': ea_dovish, 'ru_hawkish': ru_hawkish, 'ru_dovish': ru_dovish, 'em_hawkish': em_hawkish, 'em_dovish': em_dovish, 'lm_hawkish': lm_hawkish, 'lm_dovish': lm_dovish, 'uem_hawkish': uem_hawkish, 'uem_dovish': uem_dovish}\n",
    "            total_hawk = cpi_hawkish + coreinflation_hawkish + ip_inflation_hawkish + cs_hawkish + e_hawkish + ea_hawkish + ru_hawkish + em_hawkish + lm_hawkish + uem_hawkish\n",
    "            total_dovish = cpi_dovish + coreinflation_dovish + ip_inflation_dovish + cs_dovish + e_dovish + ea_dovish + ru_dovish + em_dovish + lm_dovish + uem_dovish\n",
    "            if(total_hawk + total_dovish == 0):\n",
    "                if(os.path.exists(file_name + '.pdf')):\n",
    "                    os.remove(file_name + '.pdf')\n",
    "                if(os.path.exists(file_name + '.png')):\n",
    "                    os.remove(file_name + '.png')\n",
    "                print('bye bye bitch')\n",
    "                continue\n",
    "            apel_net_sentiment =1 + (total_hawk - total_dovish)/(total_hawk + total_dovish)\n",
    "            new_net_sentiment = 100*(total_hawk - total_dovish)/(.5*N)\n",
    "            dict['apel_net_sentiment'] = apel_net_sentiment\n",
    "            dict['new_net_sentiment'] = new_net_sentiment\n",
    "            with open(file_name + '.json', 'w') as fp:\n",
    "                json.dump(dict, fp, indent=4)\n",
    "            \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uttkarsh/IP\n",
      "['/home/uttkarsh/IP/Governor Speeches/2004 Q2', '/home/uttkarsh/IP/Governor Speeches/2005 Q1', '/home/uttkarsh/IP/Governor Speeches/2008 Q3', '/home/uttkarsh/IP/Governor Speeches/2021 Q1', '/home/uttkarsh/IP/Governor Speeches/2018 Q1', '/home/uttkarsh/IP/Governor Speeches/2014 Q1', '/home/uttkarsh/IP/Governor Speeches/2019 Q3', '/home/uttkarsh/IP/Governor Speeches/2014 Q2', '/home/uttkarsh/IP/Governor Speeches/2017 Q4', '/home/uttkarsh/IP/Governor Speeches/2007 Q1', '/home/uttkarsh/IP/Governor Speeches/2011 Q3', '/home/uttkarsh/IP/Governor Speeches/2005 Q3', '/home/uttkarsh/IP/Governor Speeches/2009 Q4', '/home/uttkarsh/IP/Governor Speeches/2023 Q2', '/home/uttkarsh/IP/Governor Speeches/2011 Q2', '/home/uttkarsh/IP/Governor Speeches/2022 Q3', '/home/uttkarsh/IP/Governor Speeches/2011 Q1', '/home/uttkarsh/IP/Governor Speeches/2016 Q1', '/home/uttkarsh/IP/Governor Speeches/2023 Q1', '/home/uttkarsh/IP/Governor Speeches/2009 Q3', '/home/uttkarsh/IP/Governor Speeches/2016 Q2', '/home/uttkarsh/IP/Governor Speeches/2016 Q3', '/home/uttkarsh/IP/Governor Speeches/2022 Q1', '/home/uttkarsh/IP/Governor Speeches/2012 Q3', '/home/uttkarsh/IP/Governor Speeches/2009 Q2', '/home/uttkarsh/IP/Governor Speeches/2018 Q4', '/home/uttkarsh/IP/Governor Speeches/2005 Q4', '/home/uttkarsh/IP/Governor Speeches/2004 Q3', '/home/uttkarsh/IP/Governor Speeches/2005 Q2', '/home/uttkarsh/IP/Governor Speeches/2015 Q2', '/home/uttkarsh/IP/Governor Speeches/2007 Q4', '/home/uttkarsh/IP/Governor Speeches/2010 Q4', '/home/uttkarsh/IP/Governor Speeches/2021 Q3', '/home/uttkarsh/IP/Governor Speeches/2009 Q1', '/home/uttkarsh/IP/Governor Speeches/2012 Q1', '/home/uttkarsh/IP/Governor Speeches/2011 Q4', '/home/uttkarsh/IP/Governor Speeches/2007 Q2', '/home/uttkarsh/IP/Governor Speeches/2013 Q4', '/home/uttkarsh/IP/Governor Speeches/2010 Q3', '/home/uttkarsh/IP/Governor Speeches/2023 Q3', '/home/uttkarsh/IP/Governor Speeches/2010 Q2', '/home/uttkarsh/IP/Governor Speeches/2013 Q3', '/home/uttkarsh/IP/Governor Speeches/2020 Q1', '/home/uttkarsh/IP/Governor Speeches/2013 Q1', '/home/uttkarsh/IP/Governor Speeches/2019 Q2', '/home/uttkarsh/IP/Governor Speeches/2010 Q1', '/home/uttkarsh/IP/Governor Speeches/2020 Q3', '/home/uttkarsh/IP/Governor Speeches/2008 Q2', '/home/uttkarsh/IP/Governor Speeches/2008 Q1']\n",
      "/home/uttkarsh/IP/Governor Speeches/2004 Q2\n",
      "24 Apr 04 MP.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uttkarsh/IP/Governor Speeches/2005 Q1\n",
      "12 Feb 05 MP.pdf\n",
      "08 Jan 05 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2008 Q3\n",
      "1 Aug 08 MP.pdf\n",
      "bye bye bitch\n",
      "4 July 08 MP.pdf\n",
      "4 Aug 08 FM.pdf\n",
      "bye bye bitch\n",
      "/home/uttkarsh/IP/Governor Speeches/2021 Q1\n",
      "16 Jan 21 FSU.pdf\n",
      "bye bye bitch\n",
      "/home/uttkarsh/IP/Governor Speeches/2018 Q1\n",
      "15 Jan 18 FM.pdf\n",
      "bye bye bitch\n",
      "/home/uttkarsh/IP/Governor Speeches/2014 Q1\n",
      "26 Feb 14 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2019 Q3\n",
      "19 Aug 19 FM.pdf\n",
      "bye bye bitch\n",
      "/home/uttkarsh/IP/Governor Speeches/2014 Q2\n",
      "10 Apr 14 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2017 Q4\n",
      "27 Oct 17 MP.pdf\n",
      "16 Nov 17 MP.pdf\n",
      "20 Nov 17 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2007 Q1\n",
      "06 Mar 07 FM.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2011 Q3\n",
      "27 Sep 11 MP.pdf\n",
      "24 Sep 11 MP.pdf\n",
      "bye bye bitch\n",
      "/home/uttkarsh/IP/Governor Speeches/2005 Q3\n",
      "13 Aug 05 MP.pdf\n",
      "bye bye bitch\n",
      "3 Sep 05 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2009 Q4\n",
      "5 Oct 09 MP.pdf\n",
      "16 Oct 09 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2023 Q2\n",
      "27 Apr 23 FM.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2011 Q2\n",
      "17 Apr 11 FM.pdf\n",
      "23 June 11 MP.pdf\n",
      "10 May 11 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2022 Q3\n",
      "9 July 22 MP.pdf\n",
      "5 Sep 22 FM.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2011 Q1\n",
      "21 Feb 11 MP.pdf\n",
      "31 Jan 11 MP.pdf\n",
      "bye bye bitch\n",
      "7 Jan 11 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2016 Q1\n",
      "12 Mar 16 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2023 Q1\n",
      "27 Jan 23 FM.pdf\n",
      "27 Jan 23 FSU.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2009 Q3\n",
      "11 Sep 09 MP.pdf\n",
      "31 July 09 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2016 Q2\n",
      "20 Apr 16 MP.pdf\n",
      "bye bye bitch\n",
      "20 June 16 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2016 Q3\n",
      "26 July 16 MP.pdf\n",
      "26 Aug 16 FM.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2022 Q1\n",
      "4 Mar 22 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2012 Q3\n",
      "19 July 12 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2009 Q2\n",
      "25 Apr 09 MP.pdf\n",
      "29 June 09 MP.pdf\n",
      "22 May 09 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2018 Q4\n",
      "26 Oct 18 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2005 Q4\n",
      "4 Nov 05 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2004 Q3\n",
      "20 Aug 04 FM.pdf\n",
      "bye bye bitch\n",
      "/home/uttkarsh/IP/Governor Speeches/2005 Q2\n",
      "7 June 05 MP.pdf\n",
      "bye bye bitch\n",
      "/home/uttkarsh/IP/Governor Speeches/2015 Q2\n",
      "19 May 15 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2007 Q4\n",
      "24 Oct 07 FM.pdf\n",
      "bye bye bitch\n",
      "/home/uttkarsh/IP/Governor Speeches/2010 Q4\n",
      "8 Dec 10 MP.pdf\n",
      "27 Oct 10 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2021 Q3\n",
      "16 July 21 MP.pdf\n",
      "31 Aug 21 FM.pdf\n",
      "bye bye bitch\n",
      "/home/uttkarsh/IP/Governor Speeches/2009 Q1\n",
      "18 Feb 09 MP.pdf\n",
      "26 Mar 09 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2012 Q1\n",
      "1 Feb 12 MP.pdf\n",
      "22 Feb 12 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2011 Q4\n",
      "22 Nov 11 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2007 Q2\n",
      "9 Apr 07 FM.pdf\n",
      "28 May 07 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2013 Q4\n",
      "20 Oct 13 MP.pdf\n",
      "bye bye bitch\n",
      "/home/uttkarsh/IP/Governor Speeches/2010 Q3\n",
      "27 Aug 10 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2023 Q3\n",
      "11 Aug 23.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2010 Q2\n",
      "27 Apr 10 MP.pdf\n",
      "25 Apr 10 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2013 Q3\n",
      "29 Aug 13 MP.pdf\n",
      "18 July 13 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2020 Q1\n",
      "24 Jan 20 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2013 Q1\n",
      "3 Jan 13 MP.pdf\n",
      "13 Mar 13 MP.pdf\n",
      "8 Mar 13 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2019 Q2\n",
      "19 Mar 19 MP.pdf\n",
      "bye bye bitch\n",
      "12 Apr 19 MP.pdf\n",
      "29 June 19 FM.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2010 Q1\n",
      "24 Feb 10 MP.pdf\n",
      "2 Mar 10 MP.pdf\n",
      "bye bye bitch\n",
      "4 Jan 10 MP.pdf\n",
      "12 Feb 10 MP.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2020 Q3\n",
      "11 July 20 FSU.pdf\n",
      "/home/uttkarsh/IP/Governor Speeches/2008 Q2\n",
      "2 June 08 MP.pdf\n",
      "bye bye bitch\n",
      "17 Apr 08 FM.pdf\n",
      "bye bye bitch\n",
      "5 June 08 MP.pdf\n",
      "10 Apr 08 MP.pdf\n",
      "bye bye bitch\n",
      "/home/uttkarsh/IP/Governor Speeches/2008 Q1\n",
      "21 Jan 08 FM.pdf\n"
     ]
    }
   ],
   "source": [
    "make_wordcloud_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('mpc_mp_surprises_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise_hd = {}\n",
    "def fn(df):\n",
    "    for date in df['Meeting_Date']:\n",
    "        dates = date.split('/')\n",
    "        file = dates[0] + '-' + dates[2] + '.pdf'\n",
    "        text = get_text(file)\n",
    "        token = filter(text)\n",
    "        bigram_list = get_bigram(token)\n",
    "        trigram_list = get_trigram(token)\n",
    "        N = len(token)\n",
    "        string_list = get_string_from_list(token)\n",
    "        cpi_hawkish = searchAndCount_single(bigram_list, 'cpi', ConsumerPricesInflation_Hawkish) + searchAndCount_double(bigram_list, 'cpi', 'inflation', ConsumerPricesInflation_Hawkish) + searchAndCount_triple(trigram_list, 'consumer', 'prices', 'inflation', ConsumerPricesInflation_Hawkish) + searchAndCount_single(bigram_list, 'inflation', ConsumerPricesInflation_Hawkish)\n",
    "        cpi_dovish = searchAndCount_single(bigram_list, 'cpi', ConsumerPricesInflation_Dovish) + searchAndCount_double(bigram_list, 'cpi', 'inflation', ConsumerPricesInflation_Dovish) + searchAndCount_triple(trigram_list, 'consumer', 'prices', 'inflation', ConsumerPricesInflation_Dovish) + searchAndCount_single(bigram_list, 'inflation', ConsumerPricesInflation_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        coreinflation_hawkish = searchAndCount_double(bigram_list, 'core', 'inflation', ConsumerPricesInflation_Hawkish)\n",
    "        coreinflation_dovish = searchAndCount_double(bigram_list, 'core', 'inflation', ConsumerPricesInflation_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        ip_inflation_hawkish = searchAndCount_double(bigram_list, 'inflation', 'pressure', InflationPressure_Hawkish)\n",
    "        ip_inflation_dovish = searchAndCount_double(bigram_list, 'inflation', 'pressure', InflationPressure_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        cs_hawkish = searchAndCount_double(bigram_list, 'consumer', 'spending', ConsumerSpending_Hawkish)\n",
    "        cs_dovish = searchAndCount_double(bigram_list, 'consumer', 'spending', ConsumerSpending_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        e_hawkish = searchAndCount_single(bigram_list, 'economic', EconomicActivity_Hawkish)\n",
    "        e_dovish = searchAndCount_single(bigram_list, 'economic', EconomicActivity_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        ea_hawkish = searchAndCount_double(bigram_list, 'economic', 'activity', EconomicActivity_Hawkish)\n",
    "        ea_dovish = searchAndCount_double(bigram_list, 'economic', 'activity', EconomicActivity_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        ru_hawkish = searchAndCount_double(bigram_list, 'resource', 'utilisation', ResourceUtilization_Hawkish) + searchAndCount_double(bigram_list, 'resource', 'utilization', ResourceUtilization_Hawkish)\n",
    "        ru_dovish = searchAndCount_double(bigram_list, 'resource', 'utilisation', ResourceUtilization_Dovish) + searchAndCount_double(bigram_list, 'resource', 'utilization', ResourceUtilization_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        em_hawkish = searchAndCount_single(bigram_list, 'employment', Employment_Hawkish)\n",
    "        em_dovish = searchAndCount_single(bigram_list, 'employment', Employment_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        lm_hawkish = searchAndCount_single(bigram_list, 'labour', LaborMarket_Hawkish) + searchAndCount_double(bigram_list, 'labour', 'market', LaborMarket_Hawkish)\n",
    "        lm_dovish = searchAndCount_single(bigram_list, 'labour', LaborMarket_Dovish) + searchAndCount_double(bigram_list, 'labour', 'market', LaborMarket_Dovish)\n",
    "        # --------------------------------------------------------------------------------------------\n",
    "        uem_hawkish = searchAndCount_single(bigram_list, 'unemployment', Unemployment_Hawkish)\n",
    "        uem_dovish = searchAndCount_single(bigram_list, 'unemployment', Unemployment_Dovish)\n",
    "\n",
    "        # make a dictionary of all above values and dump it in a json file\n",
    "        dict = {'cpi_hawkish': cpi_hawkish, 'cpi_dovish': cpi_dovish, 'coreinflation_hawkish': coreinflation_hawkish, 'coreinflation_dovish': coreinflation_dovish, 'ip_inflation_hawkish': ip_inflation_hawkish, 'ip_inflation_dovish': ip_inflation_dovish, 'cs_hawkish': cs_hawkish, 'cs_dovish': cs_dovish, 'e_hawkish': e_hawkish, 'e_dovish': e_dovish, 'ea_hawkish': ea_hawkish, 'ea_dovish': ea_dovish, 'ru_hawkish': ru_hawkish, 'ru_dovish': ru_dovish, 'em_hawkish': em_hawkish, 'em_dovish': em_dovish, 'lm_hawkish': lm_hawkish, 'lm_dovish': lm_dovish, 'uem_hawkish': uem_hawkish, 'uem_dovish': uem_dovish}\n",
    "        total_hawk = cpi_hawkish + coreinflation_hawkish + ip_inflation_hawkish + cs_hawkish + e_hawkish + ea_hawkish + ru_hawkish + em_hawkish + lm_hawkish + uem_hawkish\n",
    "        total_dovish = cpi_dovish + coreinflation_dovish + ip_inflation_dovish + cs_dovish + e_dovish + ea_dovish + ru_dovish + em_dovish + lm_dovish + uem_dovish\n",
    "        if(total_hawk + total_dovish == 0):\n",
    "            print('bhai is file ne to kaat dia')\n",
    "            continue\n",
    "        apel_net_sentiment =1 + (total_hawk - total_dovish)/(total_hawk + total_dovish)\n",
    "        # new_net_sentiment = 100*(total_hawk - total_dovish)/(.5*N)\n",
    "        surprise_hd[date] = {}\n",
    "        surprise_hd[date]['hd'] = (apel_net_sentiment)\n",
    "        surprise_hd[date]['surprise'] = (df[df['Meeting_Date'] == date]['MP_Surprise'].values[0])\n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10/4/2016': {'hd': (0.5147058823529411, -7.166123778501628), 'mp': (-0.12, -0.12)}, '12/7/2016': {'hd': (1.1451612903225807, 1.3447889428464699), 'mp': (0.26, 0.26)}, '2/8/2017': {'hd': (0.8764044943820225, -1.4920311970159377), 'mp': (0.13, 0.13)}, '4/6/2017': {'hd': (0.9868421052631579, -0.1139925904816187), 'mp': (0.02, 0.02)}, '6/7/2017': {'hd': (0.7037037037037037, -3.0274361400189216), 'mp': (0.02, 0.02)}, '8/2/2017': {'hd': (0.4031007751937985, -7.464857004362579), 'mp': (-0.06, -0.06)}, '10/4/2017': {'hd': (1.4052044609665426, 5.330073349633252), 'mp': (0.03, 0.03)}, '12/6/2017': {'hd': (1.0471204188481675, 0.47581284694686754), 'mp': (-0.01, -0.01)}, '2/7/2018': {'hd': (1.5969387755102042, 13.382899628252789), 'mp': (-0.03, -0.03)}, '4/5/2018': {'hd': (1.1467391304347827, 2.6620655656889327), 'mp': (-0.015, -0.015)}, '6/6/2018': {'hd': (1.5328947368421053, 8.50170558908423), 'mp': (0.045, 0.045)}, '8/1/2018': {'hd': (1.2798053527980535, 5.5582406959884), 'mp': (0.09, 0.09)}, '10/5/2018': {'hd': (1.2692307692307692, 3.298774740810556), 'mp': (-0.285, -0.285)}, '12/5/2018': {'hd': (0.9794520547945206, -0.272789270288702), 'mp': (0.0, 0.0)}, '2/7/2019': {'hd': (0.25806451612903225, -9.860664523043944), 'mp': (-0.15, -0.15)}, '4/4/2019': {'hd': (1.0, 0.0), 'mp': (0.02, 0.02)}, '6/6/2019': {'hd': (0.0, -3.8135593220338984), 'mp': (-0.02, -0.02)}, '8/7/2019': {'hd': (0.7058823529411764, -0.8025682182985554), 'mp': (-0.04, -0.04)}, '10/4/2019': {'hd': (2.0, 1.5984015984015985), 'mp': (0.02, 0.02)}, '12/5/2019': {'hd': (1.5483870967741935, 2.3792862141357594), 'mp': (0.23, 0.23)}, '2/6/2020': {'hd': (1.5897435897435899, 2.579921480650589), 'mp': (0.0, 0.0)}, '3/27/2020': {'hd': (0.0, -1.4682747771368643), 'mp': (-0.42, -0.42)}, '5/22/2020': {'hd': (0.5333333333333333, -0.72840790842872), 'mp': (-0.15, -0.15)}}\n"
     ]
    }
   ],
   "source": [
    "print(surprise_hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump to json\n",
    "with open('surprise_hd.json', 'w') as fp:\n",
    "    json.dump(surprise_hd, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 hd  surprise\n",
      "10/4/2016  0.514706    -0.120\n",
      "12/7/2016  1.145161     0.260\n",
      "2/8/2017   0.876404     0.130\n",
      "4/6/2017   0.986842     0.020\n",
      "6/7/2017   0.703704     0.020\n",
      "8/2/2017   0.403101    -0.060\n",
      "10/4/2017  1.405204     0.030\n",
      "12/6/2017  1.047120    -0.010\n",
      "2/7/2018   1.596939    -0.030\n",
      "4/5/2018   1.146739    -0.015\n",
      "6/6/2018   1.532895     0.045\n",
      "8/1/2018   1.279805     0.090\n",
      "10/5/2018  1.269231    -0.285\n",
      "12/5/2018  0.979452     0.000\n",
      "2/7/2019   0.258065    -0.150\n",
      "4/4/2019   1.000000     0.020\n",
      "6/6/2019   0.000000    -0.020\n",
      "8/7/2019   0.705882    -0.040\n",
      "10/4/2019  2.000000     0.020\n",
      "12/5/2019  1.548387     0.230\n",
      "2/6/2020   1.589744     0.000\n",
      "3/27/2020  0.000000    -0.420\n",
      "5/22/2020  0.533333    -0.150\n"
     ]
    }
   ],
   "source": [
    "# make this dictionary into a dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(surprise_hd, orient='index')\n",
    "df.to_csv('surprise_hd.csv')\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
